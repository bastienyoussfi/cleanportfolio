const PostsData = [
    { 
        id:1, 
        title: "Neural Networks Foundations", 
        link: "https://south-crowley-f39.notion.site/Neural-Networks-Foundations-411e2a54c3f34898b97c97b50ee8fdd9?pvs=4",
        date: "Jun.2024", 
        summary: "Let us try to come back to the foundations of a neural network. Simply, a neural network is a mathematical function with parameters that is trying to fit a lot of data.", 
        content: "Then, the idea is that if we know the gradient of our loss function, then we know in what direction we should move the parameters to have a better result. If, say, the parameter *a* has a negative gradient, then we know that increasing *a* will decrease our loss function. As our goal is to get our loss function as low as possible, then we know that's what we need to do.\n So, we find the gradient of our loss function for each of our parameters, and then adjust our parameters a bit in the *opposite* direction to the sign of the gradient." 
    },
    { 
        id:2, 
        title: "Natural Language Processing", 
        link: "https://south-crowley-f39.notion.site/Natural-Language-Processing-2c62781192754c0a81281d53921f454e?pvs=4",
        date: "Jun.2024", 
        summary: "Let us try to come back to the foundations of a neural network. Simply, a neural network is a mathematical function with parameters that is trying to fit a lot of data.", 
        content: "Then, the idea is that if we know the gradient of our loss function, then we know in what direction we should move the parameters to have a better result. If, say, the parameter *a* has a negative gradient, then we know that increasing *a* will decrease our loss function. As our goal is to get our loss function as low as possible, then we know that's what we need to do.\n So, we find the gradient of our loss function for each of our parameters, and then adjust our parameters a bit in the *opposite* direction to the sign of the gradient." 
    },
    { 
        id:3, 
        title: "Neural net from scratch", 
        link: "https://south-crowley-f39.notion.site/Linear-model-and-neural-net-from-scratch-0c8c7077b8a4453b956831c6a42dd4eb?pvs=4",
        date: "Jun.2024", 
        summary: "Let us try to come back to the foundations of a neural network. Simply, a neural network is a mathematical function with parameters that is trying to fit a lot of data.", 
        content: "Then, the idea is that if we know the gradient of our loss function, then we know in what direction we should move the parameters to have a better result. If, say, the parameter *a* has a negative gradient, then we know that increasing *a* will decrease our loss function. As our goal is to get our loss function as low as possible, then we know that's what we need to do.\n So, we find the gradient of our loss function for each of our parameters, and then adjust our parameters a bit in the *opposite* direction to the sign of the gradient." 
    },
    { 
        id:4, 
        title: "Collaborative Filtering", 
        link: "https://south-crowley-f39.notion.site/Collaborative-Filtering-2ad59748fd3a4711ae4964ee7933327c?pvs=4",
        date: "Jun.2024", 
        summary: "Let us try to come back to the foundations of a neural network. Simply, a neural network is a mathematical function with parameters that is trying to fit a lot of data.", 
        content: "Then, the idea is that if we know the gradient of our loss function, then we know in what direction we should move the"
    },
    { 
        id:5, 
        title: "Blog Web Application", 
        link: "https://south-crowley-f39.notion.site/Blog-Web-Application-18220d95b50b4d6cb6b8f93daae77ad8?pvs=4",
        date: "Jun.2024", 
        summary: "Let us try"
    },
    {
        id:6, 
        title: "Web Application Setup", 
        link: "https://south-crowley-f39.notion.site/Web-Application-Setup-8293f5b6ba8a4e01bf99b35960dab2c3?pvs=4",
        date: "Jun.2024", 
        summary: "Let us try"
    },
];

export default PostsData;